---
title: "STAT 8330 FALL 2015 ASSIGNMENT 1"
author: "Peng Shao"
date: "September 6, 2015"
output: 
    pdf_document:
        includes:
            in_header: mystyles.sty
---
```{r, echo = FALSE}
par(mar = c(3, 3, 3, 3))
```

$\blacktriangleright$ \textbf{Exercises 2.5.\quad Solution.} 

(1). \begin{itemize}
\item advantage: can fit many different functional forms; low bias; usually predict more accurately
\item disadvantage: overfitting problem; sually hard to interpret; high variance
\end{itemize}

(2). If our goal is to predict more accurately, it will usually be best to choose a more flexible approach.

(3). If our goal is to make some inferences, we prefer choosing a less flexible approach because the relation between response and predictor is more explicit.

$\blacktriangleright$ \textbf{Exercises 2.6.\quad Solution.} 

(1). The essential difference between parametric and non-parametric approach is that, the parametric make an assumption of the form of $f$, which can reduce problem of estimating $f$ down to one of estimating a set of parameter, but non-parametric do not make explicit assumptions about the functional form of $f$.


(2). \begin{itemize}
\item advantage: it is easier to estimate parameter; the relation between response and predictor is more explicit; 
\item disadvantage: the model we choose will usually not match the true unknown form of $f$; sometimes need more assumption.
\end{itemize}






$\blacktriangleright$ \textbf{Exercises 2.10.\quad Solution.} 



$\blacktriangleright$ \textbf{Exercises 3.5.\quad Solution.} 






$\blacktriangleright$ \textbf{Exercises 3.15.\quad Solution.}






$\blacktriangleright$ \textbf{Exercises 4.3.\quad Solution.}


We know that we classify $X$ into $k$th class based on Bayes' classifier if
$$
{p_k (x)}=\frac{f_k(x)\pi_k}{\sum_{l=1}^{K}f_l(x)\pi_l}
$$
is largest among all $p_l(x),\, l=1,2,...,K$. For 1 dimension, the density of $x$ from $k$th class is 
$$
{f_k(x)}={\frac{1}{\sqrt{2\pi}\sigma_k}}e^{-\frac{(x-\mu_k)^2}{2\sigma_k^2}} 
$$
In comparing two classes $k$ and $l$, it is sufficient to look at the log-ratio, and we see that
$$
\begin{aligned}
\log\left(\frac{p_k(x)}{p_l(x)}\right) & = \log\left(\frac{\pi_k}{\pi_l}\right) + \log\left(\frac{f_k(x)}{f_l(x)}\right)\\
& = \log\left(\frac{\pi_k}{\pi_l}\right) + \log\left(\frac{\sigma_l}{\sigma_k}\right) - \frac{(x-\mu_k)^2}{2\sigma_k^2} + \frac{(x-\mu_l)^2}{2\sigma_l^2}\\
& = \left(-\frac{(x-\mu_k)^2}{2\sigma_k^2}-\log\sigma_k+\log\pi_k\right) - \left(-\frac{(x-\mu_l)^2}{2\sigma_l^2}-\log\sigma_l+\log\pi_l\right)\\
& = \delta_k(x) - \delta_l(x)
\end{aligned}
$$
Then the Beyes' classifier can be be defined as 
$$
C(x)=\argmax_k\delta_k(x)
$$
where $\delta_k(x) = -\frac{(x-\mu_k)^2}{2\sigma_k^2}-\log\sigma_k+\log\pi_k$.

It is obvious that the decision boundary between each pair of classes k and l is described by a quadratic equation $\{x : \delta_k(x) = \delta_l(x)\}$. 






$\blacktriangleright$ \textbf{Exercises 4.10.\quad Solution.} 



(a). From the output, we can see that (1) the variable Volume is increased as the Year increased, and the increase rates become larger and larger; (2) the variable Today is highly, but not complete, correlated with the indicator variable Direction, so we may guess that Direction is transformed from Today. Except this two pairs, no other pairs show any obvious patterns.
```{r, echo=FALSE}
rm(list = ls())
library(ISLR)
library(ggplot2)
attach(Weekly)
```

```{r}
cor(Weekly[, -9])
pairs(Weekly, col = Direction)
```

(b). Fitting the model as below, the summary result shows that only intercept and coefficient of Lag2 is significant.
```{r}
logit.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                family = binomial, data = Weekly)
summary(logit.fit)
```

(c). Using threshold = 0.5, 
```{r, echo = FALSE}
ConfusionTable <- function(ct){
    accuracy <- (ct[1, 1] + ct[2, 2]) / (sum(ct))
    TPr <- ct[2, 2] / (ct[1, 2] + ct[2, 2])
    FPr <- ct[2, 1] / (ct[1, 1] + ct[2, 1])
    precision <- ct[2, 2] / (ct[2, 1] + ct[2, 2])
    error <- 1 - accuracy
    result <- list(accuracy, TPr, FPr, precision, error)
    names(result) <- c("Accuracy", "True Positive Rate", 
                       "False Posistive Rate", "Precision", 
                       "Total Error Rate")
    return(result)
}
```

```{r}
glm.probs <- predict(logit.fit,type="response")
glm.pred <- rep("Down",nrow(Weekly))
glm.pred[glm.probs > 0.50]="Up"
ct <- table(glm.pred, Direction)
ct
ConfusionTable(ct)
```



$\blacktriangleright$ \textbf{Exercises 4.13.\quad Solution.} 




$\blacktriangleright$ \textbf{Appendices} 


\textbf{Code of function ConfusionTable()} 
```{r, eval = FALSE}
ConfusionTable <- function(ct){
    accuracy <- (ct[1, 1] + ct[2, 2]) / (sum(ct))
    TPr <- ct[2, 2] / (ct[1, 2] + ct[2, 2])
    FPr <- ct[2, 1] / (ct[1, 1] + ct[2, 1])
    precision <- ct[2, 2] / (ct[2, 1] + ct[2, 2])
    error <- 1 - accuracy
    result <- list(accuracy, TPr, FPr, precision, error)
    names(result) <- c("Accuracy", "True Positive Rate", 
                       "False Posistive Rate", "Precision", 
                       "Total Error Rate")
    return(result)
}
```

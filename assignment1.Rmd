---
title: "STAT 8330 FALL 2015 ASSIGNMENT 1"
author: "Peng Shao"
date: "September 6, 2015"
output: 
    pdf_document:
        includes:
            in_header: mystyles.sty
---
```{r, echo = FALSE}
par(mar = c(3, 3, 3, 3))
```

$\blacktriangleright$ \textbf{Exercises 2.5.\quad Solution.} 

(1). \begin{itemize}
\item advantage: can fit many different functional forms; low bias; usually predict more accurately
\item disadvantage: overfitting problem; sually hard to interpret; high variance
\end{itemize}

(2). If our goal is to predict more accurately, it will usually be best to choose a more flexible approach.

(3). If our goal is to make some inferences, we prefer choosing a less flexible approach because the relation between response and predictor is more explicit.

$\blacktriangleright$ \textbf{Exercises 2.6.\quad Solution.} 

(1). The essential difference between parametric and non-parametric approach is that, the parametric make an assumption of the form of $f$, which can reduce problem of estimating $f$ down to one of estimating a set of parameter, but non-parametric do not make explicit assumptions about the functional form of $f$.


(2). \begin{itemize}
\item advantage: it is easier to estimate parameter; the relation between response and predictor is more explicit; 
\item disadvantage: the model we choose will usually not match the true unknown form of $f$; sometimes need more assumption.
\end{itemize}






$\blacktriangleright$ \textbf{Exercises 2.10.\quad Solution.} 



$\blacktriangleright$ \textbf{Exercises 3.5.\quad Solution.} 






$\blacktriangleright$ \textbf{Exercises 3.15.\quad Solution.}






$\blacktriangleright$ \textbf{Exercises 4.3.\quad Solution.}


We know that we classify $X$ into $k$th class based on Bayes' classifier if
$$
{p_k (x)}=\frac{f_k(x)\pi_k}{\sum_{l=1}^{K}f_l(x)\pi_l}
$$
is largest among all $p_l(x),\, l=1,2,...,K$. For 1 dimension, the density of $x$ from $k$th class is 
$$
{f_k(x)}={\frac{1}{\sqrt{2\pi}\sigma_k}}e^{-\frac{(x-\mu_k)^2}{2\sigma_k^2}} 
$$
In comparing two classes $k$ and $l$, it is sufficient to look at the log-ratio, and we see that
$$
\begin{aligned}
\log\left(\frac{p_k(x)}{p_l(x)}\right) & = \log\left(\frac{\pi_k}{\pi_l}\right) + \log\left(\frac{f_k(x)}{f_l(x)}\right)\\
& = \log\left(\frac{\pi_k}{\pi_l}\right) + \log\left(\frac{\sigma_l}{\sigma_k}\right) - \frac{(x-\mu_k)^2}{2\sigma_k^2} + \frac{(x-\mu_l)^2}{2\sigma_l^2}\\
& = \left(-\frac{(x-\mu_k)^2}{2\sigma_k^2}-\log\sigma_k+\log\pi_k\right) - \left(-\frac{(x-\mu_l)^2}{2\sigma_l^2}-\log\sigma_l+\log\pi_l\right)\\
& = \delta_k(x) - \delta_l(x)
\end{aligned}
$$
Then the Beyes' classifier can be be defined as 
$$
C(x)=\argmax_k\delta_k(x)
$$
where $\delta_k(x) = -\frac{(x-\mu_k)^2}{2\sigma_k^2}-\log\sigma_k+\log\pi_k$.

It is obvious that the decision boundary between each pair of classes k and l is described by a quadratic equation $\{x : \delta_k(x) = \delta_l(x)\}$. 






$\blacktriangleright$ \textbf{Exercises 4.10.\quad Solution.} 



(a). From the output, we can see that (1) the variable Volume is increased as the Year increased, and the increase rates become larger and larger; (2) the variable Today is highly, but not complete, correlated with the indicator variable Direction, so we may guess that Direction is transformed from Today. Except this two pairs, no other pairs show any obvious patterns.
```{r, echo=FALSE}
rm(list = ls())
library(ISLR)
library(ggplot2)
library(MASS)
library(class)
library(pROC)
attach(Weekly)
```

```{r}
cor(Weekly[, -9])
pairs(Weekly, col = Direction)
```

(b). Fitting the model as below, the summary result shows that only intercept and coefficient of Lag2 is significant.
```{r}
logit.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                family = binomial, data = Weekly)
summary(logit.fit)
```

(c). Using threshold = 0.5, we can get the confusion table as below. At a first glance, the prediction accuracy is 56.11% and the error rate is 43.89%, which is not so good but acceptable, since the prediction of trend of stock index is so difficult. The true positive rate is so good as 92.07%, but the false positive is also too high (almost 90%), which is catastrophic. So we should review the accuracy and error rate. Suppose we have a trivial classifier which always predict "UP". Then the error rate of this classifier is $484/1089\times 100\%=44.44\%$, which is just slightly worse than the logistic regression! So, the logsitic regression using all variables as predictors is almost useless for this case.
```{r, echo = FALSE}
ConfusionTable <- function(ct){
    accuracy <- (ct[1, 1] + ct[2, 2]) / (sum(ct))
    TPr <- ct[2, 2] / (ct[1, 2] + ct[2, 2])
    FPr <- ct[2, 1] / (ct[1, 1] + ct[2, 1])
    precision <- ct[2, 2] / (ct[2, 1] + ct[2, 2])
    error <- 1 - accuracy
    result <- list(accuracy, TPr, FPr, precision, error)
    names(result) <- c("Accuracy", "True Positive Rate", 
                       "False Posistive Rate", "Precision", 
                       "Total Error Rate")
    return(result)
}
```

```{r}
glm.probs <- predict(logit.fit,type="response")
glm.pred <- rep("Down",nrow(Weekly))
glm.pred[glm.probs > 0.50] <- "Up"
ct <- table(glm.pred, Direction)
ct
ConfusionTable(ct)
```

(d). The overall fraction of correct predictions is the accuracy of the classifier, which is 62.5%.
```{r}
train <- Year <= 2008
Weekly.test <- Weekly[!train, ]
logit.fit <- glm(Direction ~ Lag2, family = binomial, data = Weekly, subset = train)
glm.probs <- predict(logit.fit, Weekly.test, type="response")
glm.pred <- rep("Down", nrow(Weekly.test))
glm.pred[glm.probs > 0.50] <- "Up"
ct <- table(glm.pred, Weekly.test$Direction)
ct
ConfusionTable(ct)$Accuracy
```

(e). The overall fraction of correct predictions is 62.5%.
```{r}
lda.fit <- lda(Direction ~ Lag2, data = Weekly, subset = train)
lda.class <- predict(lda.fit, Weekly.test)$class
ct <- table(lda.class,Weekly.test$Direction)
ct
ConfusionTable(ct)$Accuracy
```

(f).The overall fraction of correct predictions is 58.65%.
```{r}
qda.fit <- qda(Direction ~ Lag2, data = Weekly, subset = train)
qda.class <- predict(qda.fit, Weekly.test)$class
ct <- table(qda.class, Weekly.test$Direction)
ct
ConfusionTable(ct)$Accuracy
```

(g). The overall fraction of correct predictions is 50%.
```{r}
train.X <- matrix(Lag2[train])
test.X <- matrix(Lag2[!train])
train.Direction <- Direction[train]
test.Direction <- Direction[!train]
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Direction, k = 1)
ct <- table(knn.pred, test.Direction)
ct
ConfusionTable(ct)$Accuracy
```

(h) To compare the these four methods, the simplest way is to plot the ROC curve of each method. Plots are list below, ans R can also provide the AUCs of four methods. We can see that QDA (the third one) has the largest AUC, which is 0.4914. Thus, we can say that QDA may be the best classifier among these four methods for this dataset.
```{r, echo = FALSE}
par(mfrow = c(2, 2))
glm.probs <- predict(logit.fit, Weekly.test, type="response")
Weekly.test$prob <- glm.probs
g <- roc(Direction ~ prob, data = Weekly.test)
plot(g, main = "Logistic Regression")
lda.probs <- predict(lda.fit, Weekly.test)$posterior
Weekly.test$prob <- lda.probs[, 2]
g <- roc(Direction ~ prob, data = Weekly.test)
plot(g, main = "LDA")
qda.probs <- predict(qda.fit, Weekly.test)$posterior
Weekly.test$prob <- qda.probs[, 2]
g <- roc(Direction ~ prob, data = Weekly.test)
plot(g, main = "QDA")
knn.pred <- knn(train.X, test.X, train.Direction, k = 1, prob = TRUE)
prob <- attr(knn.pred, "prob")
prob <- ifelse(knn.pred == "Down", 1-prob, prob)
Weekly.test <- data.frame(test.Direction, prob)
g <- roc(test.Direction ~ prob, data = Weekly.test)
plot(g, main = "KNN")

```






$\blacktriangleright$ \textbf{Exercises 4.13.\quad Solution.} 




$\blacktriangleright$ \textbf{Appendices} 


\textbf{Code of function ConfusionTable()} 
```{r, eval = FALSE}
ConfusionTable <- function(ct){
    accuracy <- (ct[1, 1] + ct[2, 2]) / (sum(ct))
    TPr <- ct[2, 2] / (ct[1, 2] + ct[2, 2])
    FPr <- ct[2, 1] / (ct[1, 1] + ct[2, 1])
    precision <- ct[2, 2] / (ct[2, 1] + ct[2, 2])
    error <- 1 - accuracy
    result <- list(accuracy, TPr, FPr, precision, error)
    names(result) <- c("Accuracy", "True Positive Rate", 
                       "False Posistive Rate", "Precision", 
                       "Total Error Rate")
    return(result)
}
```

---
title: "STAT 8330 FALL 2015 ASSIGNMENT 1"
author: "Peng Shao"
date: "September 6, 2015"
output: 
    pdf_document:
        includes:
            in_header: mystyles.sty
---
```{r, echo = FALSE}
par(mar = c(3, 3, 3, 3))
```

$\blacktriangleright$ \textbf{Exercises 2.5.\quad Solution.} 

(1). \begin{itemize}
\item advantage: can fit many different functional forms; low bias; usually predict more accurately
\item disadvantage: overfitting problem; sually hard to interpret; high variance
\end{itemize}

(2). If our goal is to predict more accurately, it will usually be best to choose a more flexible approach.

(3). If our goal is to make some inferences, we prefer choosing a less flexible approach because the relation between response and predictor is more explicit.

$\blacktriangleright$ \textbf{Exercises 2.6.\quad Solution.} 

(1). The essential difference between parametric and non-parametric approach is that, the parametric make an assumption of the form of $f$, which can reduce problem of estimating $f$ down to one of estimating a set of parameter, but non-parametric do not make explicit assumptions about the functional form of $f$.


(2). \begin{itemize}
\item advantage: it is easier to estimate parameter; the relation between response and predictor is more explicit; 
\item disadvantage: the model we choose will usually not match the true unknown form of $f$; sometimes need more assumption.
\end{itemize}






$\blacktriangleright$ \textbf{Exercises 2.10.\quad Solution.} 

(a). 
```{r}
library(MASS)
attach(Boston)
?Boston
```
The Boston data frame has 506 rows and 14 columns.
Each row represents an observation of a town. The meaning of columns are
$$
\begin{aligned}
crim&:\text{per capita crime rate by town}\\
zn&:\text{proportion of residential land zoned for lots over 25,000 sq.ft}\\
indus&:\text{proportion of non-retail business acres per town}\\
chas&:\text{Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)}\\
nox&:\text{nitrogen oxides concentration (parts per 10 million)}\\
rm&:\text{average number of rooms per dwelling}\\
age&:\text{proportion of owner-occupied units built prior to 1940}\\
dis&:\text{weighted mean of distances to five Boston employment centres}\\
rad&:\text{index of accessibility to radial highways}\\
tax&:\text{full-value property-tax rate per \$10,000}\\
ptratio&:\text{pupil-teacher ratio by town}\\
black&:\text{$1000(Bk - 0.63)^2$ where $Bk$ is the proportion of blacks by town}\\
lstat&:\text{lower status of the population (percent)}\\
medv&:\text{median value of owner-occupied homes in \$1000s}
\end{aligned}
$$

(b). 
```{r, echo = FALSE}
par(mar = c(0.5, 0.5, 0.5, 0.5))
```

```{r}
pairs(Boston, pch = 20, gap = 0, cex = 0.2)
```
We can see that nox, age and dis seems to have strong relationship with each other, and rm, lstat and medv also have strong relationship in the plots. The other pattern seem not to be that apparent.


(c).
```{r}
cor(Boston[, 1], Boston[, -1])
```
We can see that the rad and tax are relatively highly correlated with crime rate, with 0.63 and 0.58 correlation coefficient respectively. Both of these two correlations are positive.

(d). Yes. No. Almost No.

From the summaries and the boxplots, we can know that crim, zn and medv are right skewed; black is left skewed; rm is fat-tailed.
```{r}
summary(Boston)
library(ggplot2)
library(reshape)
B <- melt(as.data.frame(scale(Boston)))
ggplot(B,aes(x = variable,y = value)) + geom_boxplot()
```

(e)
```{r}
sum(chas == 1)
```



(f) From part (d)., the median of pupil-teacher ratio is 19.50.



(g)
```{r}
ind <- which(medv == min(medv))
ind
Boston[ind, ]
```
The suburbs with lowest median value of owneroccupied homes also have relatively high crime, large population of lower status and proportion of non-retail business acres, which indicates that these suburbs are kind of less developed areas.



(i) 
The number of the suburbs average more than seven rooms per dwelling is:
```{r}
sum(rm > 7)
```

The number of the suburbs average more than eight rooms per dwelling is:
```{r}
sum(rm > 8)
```
Because the maximum of rm is 8.780, then the the suburbs average more than eight rooms per dwelling can be marked as outlier for this dataset.






$\blacktriangleright$ \textbf{Exercises 3.5.\quad Solution.} 

It is very easy to get the result by using hat matrix. Consider the model
$$
\mathbf{Y}=\mathbf{X}\beta
$$
where $\mathbf{Y}=[y_1, ..., y_n]^T$, $\mathbf{X}=[x_1, ..., x_n]^T$. Then
$$
\hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}=\left(\sum_{i=1}^nx_iy_i\right)/\left(\sum_{j=1}^nx_{j}^2\right)
$$
so
$$
\hat{Y}=\mathbf{X}\hat{\beta}\left(\sum_{i=1}^nx_iy_i\right)/\left(\sum_{j=1}^nx_{j}^2\right)
$$
and
$$
\hat{y_i}=x_i\left(\sum_{i'=1}^nx_{i'}\right)/\left(\sum_{j=1}^nx_{j}^2\right)y_{i'}=\sum_{i'=1}^n\left(x_ix_{i'}\right)/\left(\sum_{j=1}^nx_{j}^2\right)y_{i'}
$$
Thus, 
$$
a_{i'}=\left(x_ix_{i'}\right)/\left(\sum_{j=1}^nx_{j}^2\right)
$$






$\blacktriangleright$ \textbf{Exercises 3.15.\quad Solution.}

(a). The statistics output is so long, so I omit the full output. From the statistical results, only chas is not significant correlated with crim. From the plots, we can see that indus, nox, rm, dis, rad, tax, black, lstat and medv show relatively apperant relationship with crim.

```{r, echo = FALSE}
par(mfrow = c(4, 4), mar = c(1, 1, 1, 1))
x <- rep(0, 13)
lm.fit <- lm(formula = crim ~ zn, data = Boston)
plot(Boston$zn, Boston$crim, main = "zn", xaxt = 'n', yaxt = 'n')
abline(lm.fit)
x[1] <- lm.fit$coefficients[2]
lm.fit <- lm(formula = crim ~ indus, data = Boston)
plot(Boston$indus, Boston$crim, main = 'indus', xaxt = 'n', yaxt = 'n')
abline(lm.fit)
x[2] <- lm.fit$coefficients[2]
lm.fit <- lm(formula = crim ~ chas, data = Boston)
plot(Boston$chas, Boston$crim, main = 'chas', xaxt = 'n', yaxt = 'n')
abline(lm.fit)
x[3] <- lm.fit$coefficients[2]
lm.fit <- lm(formula = crim ~ nox, data = Boston)
plot(Boston$nox, Boston$crim, main = 'nox', xaxt = 'n', yaxt = 'n')
abline(lm.fit)
x[4] <- lm.fit$coefficients[2]
lm.fit <- lm(formula = crim ~ rm, data = Boston)
plot(Boston$rm, Boston$crim, main = 'rm', xaxt = 'n', yaxt = 'n')
abline(lm.fit)
x[5] <- lm.fit$coefficients[2]
lm.fit <- lm(formula = crim ~ age, data = Boston)
plot(Boston$age, Boston$crim, main = 'age', xaxt = 'n', yaxt = 'n')
abline(lm.fit)
x[6] <- lm.fit$coefficients[2]
lm.fit <- lm(formula = crim ~ dis, data = Boston)
plot(Boston$dis, Boston$crim, main = 'dis', xaxt = 'n', yaxt = 'n')
abline(lm.fit)
x[7] <- lm.fit$coefficients[2]
lm.fit <- lm(formula = crim ~ rad, data = Boston)
plot(Boston$rad, Boston$crim, main = 'rad', xaxt = 'n', yaxt = 'n')
abline(lm.fit)
x[8] <- lm.fit$coefficients[2]
lm.fit <- lm(formula = crim ~ tax, data = Boston)
plot(Boston$tax, Boston$crim, main = 'tax', xaxt = 'n', yaxt = 'n')
abline(lm.fit)
x[9] <- lm.fit$coefficients[2]
lm.fit <- lm(formula = crim ~ ptratio, data = Boston)
plot(Boston$ptratio, Boston$crim, main = 'ptratio', xaxt = 'n', yaxt = 'n')
abline(lm.fit)
x[10] <- lm.fit$coefficients[2]
lm.fit <- lm(formula = crim ~ black, data = Boston)
plot(Boston$black, Boston$crim, main = 'black', xaxt = 'n', yaxt = 'n')
abline(lm.fit)
x[11] <- lm.fit$coefficients[2]
lm.fit <- lm(formula = crim ~ lstat, data = Boston)
plot(Boston$lstat, Boston$crim, main = 'lstat', xaxt = 'n', yaxt = 'n')
abline(lm.fit)
x[12] <- lm.fit$coefficients[2]
lm.fit <- lm(formula = crim ~ medv, data = Boston)
plot(Boston$medv, Boston$crim, main = 'mdev', xaxt = 'n', yaxt = 'n')
abline(lm.fit)
x[13] <- lm.fit$coefficients[2]
```

(b).
```{r}
lm.fit <- lm(formula = crim ~ ., data = Boston)
summary(lm.fit)
y <- lm.fit$coefficients[-1]
```
Variable zn, variable dis, variable rad, variable black and variable medv are significant in this model, i.e., they can reject the null hypothesis of zero. Even the F-test shows that the model is useful and significant, the coefficient of determination $R^2$ is only 0.454, which means less than 50% of data can be expained by this model. We may think that this model is underfitted.

(c).Most coefficients do not change so much, but the coefficients of nox are quite different between the single regression model and the multiple regression model. This may be because there is multicollinearity between nox and some variables, then the estimate will be greatly affected by the variables which are already included in the model.

```{r, echo = FALSE}
plot(x, y)
text(x,y,labels=names(y))
abline(0, 1)
```

(d). Full outputs are omitted because of the length. According to the result of model training, indus, nox, age, dis, ptratio and medv show significant cubic trend. The others 
```{r, eval = FALSE}
lm.fit <- lm(formula = crim ~ zn + I(zn^2) + I(zn^3), data = Boston)
summary(lm.fit)
lm.fit <- lm(formula = crim ~ indus + I(indus^2) + I(indus^3), data = Boston)
summary(lm.fit)
lm.fit <- lm(formula = crim ~ chas + I(chas^2) + I(chas^3), data = Boston)
summary(lm.fit)
lm.fit <- lm(formula = crim ~ nox + I(nox^2) + I(nox^3), data = Boston)
summary(lm.fit)
lm.fit <- lm(formula = crim ~ rm + I(rm^2) + I(rm^3), data = Boston)
summary(lm.fit)
lm.fit <- lm(formula = crim ~ age + I(age^2) + I(age^3), data = Boston)
summary(lm.fit)
lm.fit <- lm(formula = crim ~ dis + I(dis^2) + I(dis^3), data = Boston)
summary(lm.fit)
lm.fit <- lm(formula = crim ~ rad + I(rad^2) + I(rad^3), data = Boston)
summary(lm.fit)
lm.fit <- lm(formula = crim ~ tax + I(tax^2) + I(tax^3), data = Boston)
summary(lm.fit)
lm.fit <- lm(formula = crim ~ ptratio + I(ptratio^2) + I(ptratio^3), data = Boston)
summary(lm.fit)
lm.fit <- lm(formula = crim ~ black + I(black^2) + I(black^3), data = Boston)
summary(lm.fit)
lm.fit <- lm(formula = crim ~ lstat + I(lstat^2) + I(lstat^3), data = Boston)
summary(lm.fit)
lm.fit <- lm(formula = crim ~ medv + I(medv^2) + I(medv^3), data = Boston)
summary(lm.fit)
```










$\blacktriangleright$ \textbf{Exercises 4.3.\quad Solution.}


We know that we classify $X$ into $k$th class based on Bayes' classifier if
$$
{p_k (x)}=\frac{f_k(x)\pi_k}{\sum_{l=1}^{K}f_l(x)\pi_l}
$$
is largest among all $p_l(x),\, l=1,2,...,K$. For 1 dimension, the density of $x$ from $k$th class is 
$$
{f_k(x)}={\frac{1}{\sqrt{2\pi}\sigma_k}}e^{-\frac{(x-\mu_k)^2}{2\sigma_k^2}} 
$$
In comparing two classes $k$ and $l$, it is sufficient to look at the log-ratio, and we see that
$$
\begin{aligned}
\log\left(\frac{p_k(x)}{p_l(x)}\right) & = \log\left(\frac{\pi_k}{\pi_l}\right) + \log\left(\frac{f_k(x)}{f_l(x)}\right)\\
& = \log\left(\frac{\pi_k}{\pi_l}\right) + \log\left(\frac{\sigma_l}{\sigma_k}\right) - \frac{(x-\mu_k)^2}{2\sigma_k^2} + \frac{(x-\mu_l)^2}{2\sigma_l^2}\\
& = \left(-\frac{(x-\mu_k)^2}{2\sigma_k^2}-\log\sigma_k+\log\pi_k\right) - \left(-\frac{(x-\mu_l)^2}{2\sigma_l^2}-\log\sigma_l+\log\pi_l\right)\\
& = \delta_k(x) - \delta_l(x)
\end{aligned}
$$
Then the Beyes' classifier can be be defined as 
$$
C(x)=\argmax_k\delta_k(x)
$$
where $\delta_k(x) = -\frac{(x-\mu_k)^2}{2\sigma_k^2}-\log\sigma_k+\log\pi_k$.

It is obvious that the decision boundary between each pair of classes k and l is described by a quadratic equation $\{x : \delta_k(x) = \delta_l(x)\}$. 






$\blacktriangleright$ \textbf{Exercises 4.10.\quad Solution.} 



(a). From the output, we can see that (1) the variable Volume is increased as the Year increased, and the increase rates become larger and larger; (2) the variable Today is highly, but not complete, correlated with the indicator variable Direction, so we may guess that Direction is transformed from Today. Except this two pairs, no other pairs show any obvious patterns.
```{r, echo=FALSE}
rm(list = ls())
library(ISLR)
library(ggplot2)
library(MASS)
library(class)
library(pROC)
attach(Weekly)
```

```{r}
cor(Weekly[, -9])
pairs(Weekly, col = Direction)
```

(b). Fitting the model as below, the summary result shows that only intercept and coefficient of Lag2 is significant.
```{r}
logit.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                family = binomial, data = Weekly)
summary(logit.fit)
```

(c). Using threshold = 0.5, we can get the confusion table as below. At a first glance, the prediction accuracy is 56.11% and the error rate is 43.89%, which is not so good but acceptable, since the prediction of trend of stock index is so difficult. The true positive rate is so good as 92.07%, but the false positive is also too high (almost 90%), which is catastrophic. So we should review the accuracy and error rate. Suppose we have a trivial classifier which always predict "UP". Then the error rate of this classifier is $484/1089\times 100\%=44.44\%$, which is just slightly worse than the logistic regression! So, the logsitic regression using all variables as predictors is almost useless for this case.
```{r, echo = FALSE}
ConfusionTable <- function(ct){
    accuracy <- (ct[1, 1] + ct[2, 2]) / (sum(ct))
    TPr <- ct[2, 2] / (ct[1, 2] + ct[2, 2])
    FPr <- ct[2, 1] / (ct[1, 1] + ct[2, 1])
    error <- 1 - accuracy
    result <- list(accuracy, TPr, FPr, error)
    names(result) <- c("Accuracy", "True Positive Rate", 
                       "False Posistive Rate", 
                       "Total Error Rate")
    return(result)
}
```

```{r}
glm.probs <- predict(logit.fit,type="response")
glm.pred <- rep("Down",nrow(Weekly))
glm.pred[glm.probs > 0.50] <- "Up"
ct <- table(glm.pred, Direction)
ct
ConfusionTable(ct)
```

(d). The overall fraction of correct predictions is the accuracy of the classifier, which is 62.5%.
```{r}
train <- Year <= 2008
Weekly.test <- Weekly[!train, ]
logit.fit <- glm(Direction ~ Lag2, family = binomial, data = Weekly, subset = train)
glm.probs <- predict(logit.fit, Weekly.test, type="response")
glm.pred <- rep("Down", nrow(Weekly.test))
glm.pred[glm.probs > 0.50] <- "Up"
ct <- table(glm.pred, Weekly.test$Direction)
ct
ConfusionTable(ct)$Accuracy
```

(e). The overall fraction of correct predictions is 62.5%.
```{r}
lda.fit <- lda(Direction ~ Lag2, data = Weekly, subset = train)
lda.class <- predict(lda.fit, Weekly.test)$class
ct <- table(lda.class,Weekly.test$Direction)
ct
ConfusionTable(ct)$Accuracy
```

(f).The overall fraction of correct predictions is 58.65%.
```{r}
qda.fit <- qda(Direction ~ Lag2, data = Weekly, subset = train)
qda.class <- predict(qda.fit, Weekly.test)$class
ct <- table(qda.class, Weekly.test$Direction)
ct
ConfusionTable(ct)$Accuracy
```

(g). The overall fraction of correct predictions is 50%.
```{r}
train.X <- matrix(Lag2[train])
test.X <- matrix(Lag2[!train])
train.Direction <- Direction[train]
test.Direction <- Direction[!train]
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Direction, k = 1)
ct <- table(knn.pred, test.Direction)
ct
ConfusionTable(ct)$Accuracy
```

(h) To compare the these four methods, the simplest way is to plot the ROC curve of each method. Plots are list below, ans R can also provide the AUCs of four methods. We can see that QDA (the third one) has the largest AUC, which is 0.4914. In this respect, we can say that QDA may be the best classifier among these four methods for this dataset.
```{r, echo = FALSE}
par(mfrow = c(2, 2))
glm.probs <- predict(logit.fit, Weekly.test, type="response")
Weekly.test$prob <- glm.probs
g <- roc(Direction ~ prob, data = Weekly.test)
plot(g, main = "Logistic Regression")
lda.probs <- predict(lda.fit, Weekly.test)$posterior
Weekly.test$prob <- lda.probs[, 2]
g <- roc(Direction ~ prob, data = Weekly.test)
plot(g, main = "LDA")
qda.probs <- predict(qda.fit, Weekly.test)$posterior
Weekly.test$prob <- qda.probs[, 2]
g <- roc(Direction ~ prob, data = Weekly.test)
plot(g, main = "QDA")
knn.pred <- knn(train.X, test.X, train.Direction, k = 1, prob = TRUE)
prob <- attr(knn.pred, "prob")
prob <- ifelse(knn.pred == "Down", 1-prob, prob)
Weekly.test <- data.frame(test.Direction, prob)
g <- roc(test.Direction ~ prob, data = Weekly.test)
plot(g, main = "KNN")

```

(i). First, I try different value of $K$ to fit the KNN models.
```{r}
set.seed(1)
knn.pred=knn(train.X,test.X,train.Direction,k=2)
ct <- table(knn.pred, test.Direction)
ct
ConfusionTable(ct)
knn.pred=knn(train.X,test.X,train.Direction,k=4)
ct <- table(knn.pred, test.Direction)
ct
ConfusionTable(ct)
knn.pred=knn(train.X,test.X,train.Direction,k=7)
ct <- table(knn.pred, test.Direction)
ct
ConfusionTable(ct)
```
We can see that when $K=4$, the KNN model performances best. But it still has relatively high value of error rates, 41.34%, which is only 2 percents lower than the useless trivial classifier.

For QDA, I try to introduce more variable, beginning with Lag1.
```{r}
Weekly.test <- Weekly[!train, ]
qda.fit <- qda(Direction ~ Lag2 + Lag1, data = Weekly, subset = train)
qda.class <- predict(qda.fit, Weekly.test)$class
ct <- table(qda.class, Weekly.test$Direction)
ct
ConfusionTable(ct)
```
Compared to model without Lag1, the full model shows worse performance, so there is no need to introduce more predictors. Similarly, LDA with more predictors also perform worse than that just including Lag2.

For logistic regression, we do not want to introduce more variable, since it does not work on LDA. So we try to introduce the high ordet term of Lag2, like $Lag2^2$ or $Lag2^3$.
```{r}
logit.fit <- glm(Direction ~ Lag2 + Lag2^2 + Lag2^3, family = binomial, data = Weekly, subset = train)
glm.probs <- predict(logit.fit, Weekly.test, type="response")
glm.pred <- rep("Down", nrow(Weekly.test))
glm.pred[glm.probs > 0.50] <- "Up"
ct <- table(glm.pred, Weekly.test$Direction)
ct
ConfusionTable(ct)
```
The result indicates that there is still no improve in this way.

$\blacktriangleright$ \textbf{Exercises 4.13.\quad Solution.} 

First, we fit the model with all candidate predictor to see which predictors are significant.
```{r, echo = FALSE}
attach(Boston)
subset1 <- c(FALSE, abs(cor(Boston[, 1], Boston[, -1])) > 0.3, TRUE)
subset2 <- c(FALSE, abs(cor(Boston[, 1], Boston[, -1])) > 0.4, TRUE)
subset3 <- c(FALSE, abs(cor(Boston[, 1], Boston[, -1])) > 0.5, TRUE)
Boston$crim01 <- as.numeric(Boston$crim > median(Boston$crim))
set.seed(1)
rands <- runif(nrow(Boston))
test <- rands > quantile(rands, 0.75)
train <- !test
Boston.train <- Boston[train, -1]
Boston.test <- Boston[test, -1]
glm.fit=glm(crim01 ~ ., data=Boston[, -1], family = binomial)
summary(glm.fit)
```
According the reuslt, we should treat these seven variables -- $zn,\ nox,\ dis,\ rad,\ tax,\ ptratio,\ black,\ medv$ -- as predictor to build the model.

We first try LDA since it is the most inflexible model among these three method.
```{r}
lda.fit <- lda(crim01 ~ zn + nox + black + dis + rad + tax + ptratio + medv, data=Boston.train)
lda.pred=predict(lda.fit,Boston.test)$class
ct <- table(lda.pred,Boston.test$crim01)
ct
ConfusionTable(ct)
```
The result is pretty good, with error rate=15.75%, true positive Rate=76.67%, and false positive=8.96%. Then we want remove the not so significant varibales $zn,\ tax,\ black,\ medv$ and refit the model.
```{r}
lda.fit <- lda(crim01 ~ nox + dis + rad + ptratio, data=Boston.train)
lda.pred=predict(lda.fit,Boston.test)$class
ct <- table(lda.pred,Boston.test$crim01)
ct
ConfusionTable(ct)
```
The model does not go worse so much, so the the model with four variables may be enough. If we continue to remove variable, for example rad, the result is like
```{r, echo = FALSE}
lda.fit <- lda(crim01 ~ nox + dis + ptratio, data=Boston.train)
lda.pred=predict(lda.fit,Boston.test)$class
ct <- table(lda.pred,Boston.test$crim01)
ct
ConfusionTable(ct)
```
The overall performance becomes better, but what is more important is the true positive rate is greatly improved, since in the problem we do not want misclassify a town with high crime rate into the group of low crime rate, which is very dangerous. So we may prefer more a model with high true positive rate than other performance indices. I also try some other changes, but none of them can perform better, so this may be the best model for LDA.

I will use similar steps above to compare the logistic regression model. Also, I fit the model with seven variables and four variables,
```{r}
logis.fit <- glm(crim01 ~ zn + nox + black + dis + rad + tax + ptratio + medv, data=Boston.train, family = binomial)
logis.probs=predict(logis.fit, Boston.test, type="response")
logis.pred <- rep(0, nrow(Boston.test))
logis.pred[logis.probs > 0.50] <- 1
ct <- table(logis.pred,Boston.test$crim01)
ct
ConfusionTable(ct)
logis.fit <- glm(crim01 ~ nox + dis + rad + ptratio, data=Boston.train, family = binomial)
logis.probs <- predict(logis.fit, Boston.test, type="response")
logis.pred <- rep(0, nrow(Boston.test))
logis.pred[logis.probs > 0.50] <- 1
ct <- table(logis.pred,Boston.test$crim01)
ct
ConfusionTable(ct)
```
The model with seven predictors performs a little better than the one with four predictors, and also a little better compared to the LDA with four predictors. It is notable that if we fit a logistic regression model with the four predictor, but use the interaction $nox*dis$ instead of the predictors themselves, we can get similar result like the model with seven predictos. So sometimes maybe we can use a nonlinear boundary in low dimension to replace the linear boundary in high dimension without reduce the performance, but it becomes easier to interpret with less predictors.

Because of the previous attempts, we should believe that the four variables --$nox,\ dis,\ rad,\ ptratio$ is enough to make a good prediction. Then we use these predictors to fit the KNN models with differen $K$.
```{r}
set.seed(1)
Boston.train.knn <- Boston.train[,c("nox","dis","rad","ptratio")]
Boston.test.knn <-  Boston.test[,c("nox","dis","rad","ptratio")]
knn.pred <- knn(Boston.train.knn,Boston.test.knn,Boston.train$crim01,k=1)
ct <- table(knn.pred, Boston.test$crim01)
ct
ConfusionTable(ct)

knn.pred=knn(Boston.train.knn,Boston.test.knn,Boston.train$crim01,k=2)
ct <- table(knn.pred, Boston.test$crim01)
ct
ConfusionTable(ct)

knn.pred=knn(Boston.train.knn,Boston.test.knn,Boston.train$crim01,k=3)
ct <- table(knn.pred, Boston.test$crim01)
ct
ConfusionTable(ct)

knn.pred=knn(Boston.train.knn,Boston.test.knn,Boston.train$crim01,k=4)
ct <- table(knn.pred, Boston.test$crim01)
ct
ConfusionTable(ct)

knn.pred=knn(Boston.train.knn,Boston.test.knn,Boston.train$crim01,k=5)
ct <- table(knn.pred, Boston.test$crim01)
ct
ConfusionTable(ct)


knn.pred=knn(Boston.train.knn,Boston.test.knn,Boston.train$crim01,k=11)
ct <- table(knn.pred, Boston.test$crim01)
ct
ConfusionTable(ct)
```
We can see that when $K=1$, the true positive rate and the accuracy are both largest. Furthermore it is much better than the other two kinds of models. We can infer that the decision boundary is very likely to be nonlinear, even more complicated that quadratic form.

$\blacktriangleright$ \textbf{Appendices} 


\textbf{Code of function ConfusionTable()} 
```{r, eval = FALSE}
ConfusionTable <- function(ct){
    accuracy <- (ct[1, 1] + ct[2, 2]) / (sum(ct))
    TPr <- ct[2, 2] / (ct[1, 2] + ct[2, 2])
    FPr <- ct[2, 1] / (ct[1, 1] + ct[2, 1])
    precision <- ct[2, 2] / (ct[2, 1] + ct[2, 2])
    error <- 1 - accuracy
    result <- list(accuracy, TPr, FPr, precision, error)
    names(result) <- c("Accuracy", "True Positive Rate", 
                       "False Posistive Rate", "Precision", 
                       "Total Error Rate")
    return(result)
}
```

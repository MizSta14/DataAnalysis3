---
title: "STAT 8330 FALL 2015 ASSIGNMENT 1"
author: "Peng Shao"
date: "September 6, 2015"
output: 
    pdf_document:
        includes:
            in_header: mystyles.sty
---
```{r, echo = FALSE}
par(mar = c(3, 3, 3, 3))
```

$\blacktriangleright$ \textbf{Exercises 2.5.\quad Solution.} 

(1). \begin{itemize}
\item advantage: can fit many different functional forms; low bias; usually predict more accurately
\item disadvantage: overfitting problem; sually hard to interpret; high variance
\end{itemize}

(2). If our goal is to predict more accurately, it will usually be best to choose a more flexible approach.

(3). If our goal is to make some inferences, we prefer choosing a less flexible approach because the relation between response and predictor is more explicit.

$\blacktriangleright$ \textbf{Exercises 2.6.\quad Solution.} 

(1). The essential difference between parametric and non-parametric approach is that, the parametric make an assumption of the form of $f$, which can reduce problem of estimating $f$ down to one of estimating a set of parameter, but non-parametric do not make explicit assumptions about the functional form of $f$.


(2). \begin{itemize}
\item advantage: it is easier to estimate parameter; the relation between response and predictor is more explicit; 
\item disadvantage: the model we choose will usually not match the true unknown form of $f$; sometimes need more assumption.
\end{itemize}






$\blacktriangleright$ \textbf{Exercises 2.10.\quad Solution.} 



$\blacktriangleright$ \textbf{Exercises 3.5.\quad Solution.} 






$\blacktriangleright$ \textbf{Exercises 3.15.\quad Solution.}






$\blacktriangleright$ \textbf{Exercises 4.3.\quad Solution.}


We know that we classify $X$ into $k$th class based on Bayes' classifier if
$$
{p_k (x)}=\frac{f_k(x)\pi_k}{\sum_{l=1}^{K}f_l(x)\pi_l}
$$
is largest among all $p_l(x),\, l=1,2,...,K$. For 1 dimension, the density of $x$ from $k$th class is 
$$
{f_k(x)}={\frac{1}{\sqrt{2\pi}\sigma_k}}e^{-\frac{(x-\mu_k)^2}{2\sigma_k^2}} 
$$
In comparing two classes $k$ and $l$, it is sufficient to look at the log-ratio, and we see that
$$
\begin{aligned}
\log\left(\frac{p_k(x)}{p_l(x)}\right) & = \log\left(\frac{\pi_k}{\pi_l}\right) + \log\left(\frac{f_k(x)}{f_l(x)}\right)\\
& = \log\left(\frac{\pi_k}{\pi_l}\right) + \log\left(\frac{\sigma_l}{\sigma_k}\right) - \frac{(x-\mu_k)^2}{2\sigma_k^2} + \frac{(x-\mu_l)^2}{2\sigma_l^2}\\
& = \left(-\frac{(x-\mu_k)^2}{2\sigma_k^2}-\log\sigma_k+\log\pi_k\right) - \left(-\frac{(x-\mu_l)^2}{2\sigma_l^2}-\log\sigma_l+\log\pi_l\right)\\
& = \delta_k(x) - \delta_l(x)
\end{aligned}
$$
Then the Beyes' classifier can be be defined as 
$$
C(x)=\argmax_k\delta_k(x)
$$
where $\delta_k(x) = -\frac{(x-\mu_k)^2}{2\sigma_k^2}-\log\sigma_k+\log\pi_k$.

It is obvious that the decision boundary between each pair of classes k and l is described by a quadratic equation $\{x : \delta_k(x) = \delta_l(x)\}$. 






$\blacktriangleright$ \textbf{Exercises 4.10.\quad Solution.} 



(a). From the output, we can see that (1) the variable Volume is increased as the Year increased, and the increase rates become larger and larger; (2) the variable Today is highly, but not complete, correlated with the indicator variable Direction, so we may guess that Direction is transformed from Today. Except this two pairs, no other pairs show any obvious patterns.
```{r, echo=FALSE}
rm(list = ls())
library(ISLR)
library(ggplot2)
library(MASS)
library(class)
library(pROC)
attach(Weekly)
```

```{r}
cor(Weekly[, -9])
pairs(Weekly, col = Direction)
```

(b). Fitting the model as below, the summary result shows that only intercept and coefficient of Lag2 is significant.
```{r}
logit.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                family = binomial, data = Weekly)
summary(logit.fit)
```

(c). Using threshold = 0.5, we can get the confusion table as below. At a first glance, the prediction accuracy is 56.11% and the error rate is 43.89%, which is not so good but acceptable, since the prediction of trend of stock index is so difficult. The true positive rate is so good as 92.07%, but the false positive is also too high (almost 90%), which is catastrophic. So we should review the accuracy and error rate. Suppose we have a trivial classifier which always predict "UP". Then the error rate of this classifier is $484/1089\times 100\%=44.44\%$, which is just slightly worse than the logistic regression! So, the logsitic regression using all variables as predictors is almost useless for this case.
```{r, echo = FALSE}
ConfusionTable <- function(ct){
    accuracy <- (ct[1, 1] + ct[2, 2]) / (sum(ct))
    TPr <- ct[2, 2] / (ct[1, 2] + ct[2, 2])
    FPr <- ct[2, 1] / (ct[1, 1] + ct[2, 1])
    precision <- ct[2, 2] / (ct[2, 1] + ct[2, 2])
    error <- 1 - accuracy
    result <- list(accuracy, TPr, FPr, precision, error)
    names(result) <- c("Accuracy", "True Positive Rate", 
                       "False Posistive Rate", "Precision", 
                       "Total Error Rate")
    return(result)
}
```

```{r}
glm.probs <- predict(logit.fit,type="response")
glm.pred <- rep("Down",nrow(Weekly))
glm.pred[glm.probs > 0.50] <- "Up"
ct <- table(glm.pred, Direction)
ct
ConfusionTable(ct)
```

(d). The overall fraction of correct predictions is the accuracy of the classifier, which is 62.5%.
```{r}
train <- Year <= 2008
Weekly.test <- Weekly[!train, ]
logit.fit <- glm(Direction ~ Lag2, family = binomial, data = Weekly, subset = train)
glm.probs <- predict(logit.fit, Weekly.test, type="response")
glm.pred <- rep("Down", nrow(Weekly.test))
glm.pred[glm.probs > 0.50] <- "Up"
ct <- table(glm.pred, Weekly.test$Direction)
ct
ConfusionTable(ct)$Accuracy
```

(e). The overall fraction of correct predictions is 62.5%.
```{r}
lda.fit <- lda(Direction ~ Lag2, data = Weekly, subset = train)
lda.class <- predict(lda.fit, Weekly.test)$class
ct <- table(lda.class,Weekly.test$Direction)
ct
ConfusionTable(ct)$Accuracy
```

(f).The overall fraction of correct predictions is 58.65%.
```{r}
qda.fit <- qda(Direction ~ Lag2, data = Weekly, subset = train)
qda.class <- predict(qda.fit, Weekly.test)$class
ct <- table(qda.class, Weekly.test$Direction)
ct
ConfusionTable(ct)$Accuracy
```

(g). The overall fraction of correct predictions is 50%.
```{r}
train.X <- matrix(Lag2[train])
test.X <- matrix(Lag2[!train])
train.Direction <- Direction[train]
test.Direction <- Direction[!train]
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Direction, k = 1)
ct <- table(knn.pred, test.Direction)
ct
ConfusionTable(ct)$Accuracy
```

(h) To compare the these four methods, the simplest way is to plot the ROC curve of each method. Plots are list below, ans R can also provide the AUCs of four methods. We can see that QDA (the third one) has the largest AUC, which is 0.4914. Thus, we can say that QDA may be the best classifier among these four methods for this dataset.
```{r, echo = FALSE}
par(mfrow = c(2, 2))
glm.probs <- predict(logit.fit, Weekly.test, type="response")
Weekly.test$prob <- glm.probs
g <- roc(Direction ~ prob, data = Weekly.test)
plot(g, main = "Logistic Regression")
lda.probs <- predict(lda.fit, Weekly.test)$posterior
Weekly.test$prob <- lda.probs[, 2]
g <- roc(Direction ~ prob, data = Weekly.test)
plot(g, main = "LDA")
qda.probs <- predict(qda.fit, Weekly.test)$posterior
Weekly.test$prob <- qda.probs[, 2]
g <- roc(Direction ~ prob, data = Weekly.test)
plot(g, main = "QDA")
knn.pred <- knn(train.X, test.X, train.Direction, k = 1, prob = TRUE)
prob <- attr(knn.pred, "prob")
prob <- ifelse(knn.pred == "Down", 1-prob, prob)
Weekly.test <- data.frame(test.Direction, prob)
g <- roc(test.Direction ~ prob, data = Weekly.test)
plot(g, main = "KNN")

```






$\blacktriangleright$ \textbf{Exercises 4.13.\quad Solution.} 

First, we fit the model with all candidate predictor to see which predictors are significant.
```{r, echo = FALSE}
attach(Boston)
subset1 <- c(FALSE, abs(cor(Boston[, 1], Boston[, -1])) > 0.3, TRUE)
subset2 <- c(FALSE, abs(cor(Boston[, 1], Boston[, -1])) > 0.4, TRUE)
subset3 <- c(FALSE, abs(cor(Boston[, 1], Boston[, -1])) > 0.5, TRUE)
Boston$crim01 <- as.numeric(Boston$crim > median(Boston$crim))
set.seed(1)
rands <- runif(nrow(Boston))
test <- rands > quantile(rands, 0.75)
train <- !test
Boston.train <- Boston[train, -1]
Boston.test <- Boston[test, -1]
glm.fit=glm(crim01 ~ ., data=Boston[, -1], family = binomial)
summary(glm.fit)
```
According the reuslt, we should treat these seven variables -- $zn,\ nox,\ dis,\ rad,\ tax,\ ptratio,\ black,\ medv$ -- as predictor to build the model.

We first try LDA since it is the most inflexible model among these three method.
```{r}
lda.fit <- lda(crim01 ~ zn + nox + black + dis + rad + tax + ptratio + medv, data=Boston.train)
lda.pred=predict(lda.fit,Boston.test)$class
ct <- table(lda.pred,Boston.test$crim01)
ct
ConfusionTable(ct)
```
The result is pretty good, with error rate=15.75%, true positive Rate=76.67%, and false positive=8.96%. Then we want remove the not so significant varibales $zn,\ tax,\ black,\ medv$ and refit the model.
```{r}
lda.fit <- lda(crim01 ~ nox + dis + rad + ptratio, data=Boston.train)
lda.pred=predict(lda.fit,Boston.test)$class
ct <- table(lda.pred,Boston.test$crim01)
ct
ConfusionTable(ct)
```
The model does not go worse so much, so the the model with four variables may be enough. If we continue to remove variable, for example rad, the result is like
```{r, echo = FALSE}
lda.fit <- lda(crim01 ~ nox + dis + ptratio, data=Boston.train)
lda.pred=predict(lda.fit,Boston.test)$class
ct <- table(lda.pred,Boston.test$crim01)
ct
ConfusionTable(ct)
```
The overall performance becomes better, but what is more important is the true positive rate is greatly improved, since in the problem we do not want misclassify a town with high crime rate into the group of low crime rate, which is very dangerous. So we may prefer more a model with high true positive rate than other performance indices. I also try some other changes, but none of them can perform better, so this may be the best model for LDA.

I will use similar steps above to compare the logistic regression model. Also, I fit the model with seven variables and four variables,
```{r}
logis.fit <- glm(crim01 ~ zn + nox + black + dis + rad + tax + ptratio + medv, data=Boston.train, family = binomial)
logis.probs=predict(logis.fit, Boston.test, type="response")
logis.pred <- rep(0, nrow(Boston.test))
logis.pred[logis.probs > 0.50] <- 1
ct <- table(logis.pred,Boston.test$crim01)
ct
ConfusionTable(ct)
logis.fit <- glm(crim01 ~ nox + dis + rad + ptratio, data=Boston.train, family = binomial)
logis.probs=predict(logis.fit, Boston.test, type="response")
logis.pred <- rep(0, nrow(Boston.test))
logis.pred[logis.probs > 0.50] <- 1
ct <- table(logis.pred,Boston.test$crim01)
ct
ConfusionTable(ct)
```
The model with seven predictors performs a little better than the one with four predictors, and also a little better compared to the LDA with four predictors. It is notable that if we fit a logistic regression model with the four predictor, but use the interaction $nox*dis$ instead of the predictors themselves, we can get similar result like the model with seven predictos. So sometimes maybe we can use a nonlinear boundary in low dimension to replace the linear boundary in high dimension without reduce the performance, and it is easier to interpret with less predictors.


$\blacktriangleright$ \textbf{Appendices} 


\textbf{Code of function ConfusionTable()} 
```{r, eval = FALSE}
ConfusionTable <- function(ct){
    accuracy <- (ct[1, 1] + ct[2, 2]) / (sum(ct))
    TPr <- ct[2, 2] / (ct[1, 2] + ct[2, 2])
    FPr <- ct[2, 1] / (ct[1, 1] + ct[2, 1])
    precision <- ct[2, 2] / (ct[2, 1] + ct[2, 2])
    error <- 1 - accuracy
    result <- list(accuracy, TPr, FPr, precision, error)
    names(result) <- c("Accuracy", "True Positive Rate", 
                       "False Posistive Rate", "Precision", 
                       "Total Error Rate")
    return(result)
}
```

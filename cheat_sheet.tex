\documentclass[12pt, plandscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}
\usepackage{bm}

\pdfinfo{
  /Title (cheat_sheet.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Peng)
  /Subject (Bayesian)
  /Keywords (cheatsheet, exam)}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
    {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
    }

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.1ex}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}


\section{R Code}

$\spadesuit$ library(MASS)\\
lda(),qda()$\approx$lm()\\
$\spadesuit$ library(class)\\
knn(train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE)\\
$\spadesuit$  library(boot)\\
cv.glm(data, glmfit, cost, K)\\
boot(data, statistic, R, sim = "ordinary")\\
$\spadesuit$  library(leaps)\\
\#\# S3 method for class 'formula'\\
regsubsets(x=, data=, nvmax=8, force.in=NULL, force.out=NULL, intercept=TRUE, \\
method=c("exhaustive", "backward", "forward", "seqrep"))\\
$\spadesuit$  library(glmnet)\\
glmnet(x, y, family=c("gaussian","binomial",\\
"poisson",
"multinomial","cox","mgaussian"), alpha = 1, lambda=NULL))\\
alpha: The elasticnet mixing parameter, with 0<=alpha<= 1. The
          penalty is defined as

                     $(1-\alpha)/2||\beta||_2^2+\alpha||\beta||_1.$           
          
          'alpha=1' is the lasso penalty, and 'alpha=0' the ridge
          penalty.\\
\#\# S3 method for class 'glmnet'\\
predict(object, newx, s = NULL,
     type=c("link",\\"response","coefficients","nonzero","class"), exact = FALSE, offset, ...)\\
newx: Matrix of new values for 'x' at which predictions are to be
          made. Must be a matrix; can be sparse as in 'Matrix' package.
          This argument is not used for
          'type=c("coefficients","nonzero")'\\
$\spadesuit$  library(pls)\\
mvr(formula, ncomp, data, subset, 
         method = pls.options()\$mvralg,
         scale = FALSE, validation = c("none", "CV", "LOO"),
         model = TRUE, x = FALSE, y = FALSE, ...)\\
     plsr(..., method = pls.options()\$plsralg)\\
     pcr(..., method = pls.options()\$pcralg)\\
     
$\spadesuit$ library(splines)\\
bs(x, df = NULL, knots = NULL, degree = 3, intercept = FALSE,
        Boundary.knots = range(x))\\
ns(x, df = NULL, knots = NULL, intercept = FALSE,
        Boundary.knots = range(x))\\
smooth.spline(x, y = NULL, w = NULL, df, spar = NULL, cv = FALSE,
                   all.knots = FALSE, nknots = .nknots.smspl,
                   keep.data = TRUE, df.offset = 0, penalty = 1,
                   control.spar = list(), tol = 1e-6 * IQR(x))\\
$\spadesuit$ library(gam)\\
gam(formula, family = gaussian, data, weights, subset, na.action, 
            start, etastart, mustart, control = gam.control(...),
     model=TRUE, method, x=FALSE, y=TRUE, ...)\\
     lo(..., span=0.5, degree=1)\\
     gam.lo(x, y, w, span, degree, ncols, xeval)\\
$\spadesuit$ library(gam)\\
tree(formula, data, weights, subset,
          na.action = na.pass, control = tree.control(nobs, ...),
          method = "recursive.partition",
          split = c("deviance", "gini"),
          model = FALSE, x = FALSE, y = TRUE, wts = TRUE, ...)\\
prune.tree(tree, k = NULL, best = NULL, newdata, nwts,
                method = c("deviance", "misclass"), loss, eps = 1e-3)\\
cv.tree(object, rand, FUN = prune.tree, K = 10, ...)\\
$\spadesuit$ library(randomForest)\\
\#\# S3 method for class 'formula'\\
     randomForest(formula, data=NULL, ..., subset, na.action=na.fail)\\
\#\# Default S3 method:\\
randomForest(x, y=NULL, ntree=500, mtry=if (!is.null(y) \&\& !is.factor(y))
                  max(floor(ncol(x)/3), 1) else floor(sqrt(ncol(x))), importance=FALSE, proximity, oob.prox=proximity)\\
\#\# S3 method for class 'randomForest'\\
     importance(x, type=NULL, class=NULL, scale=TRUE, ...)\\
$\spadesuit$ library(gbm)\\
gbm(formula = formula(data),
         distribution = "bernoulli",
         data = list(),
         n.trees = 100,
         interaction.depth = 1,
         n.minobsinnode = 10,
         shrinkage = 0.001,
         cv.folds=0,
         verbose = "CV")\\    
Currently available options are "gaussian" (squared error),
          "laplace" (absolute loss), "tdist" (t-distribution loss),
          "bernoulli" (logistic regression for 0-1 outcomes),
          "huberized" (huberized hinge loss for 0-1 outcomes),
          "multinomial" (classification when there are more than 2
          classes), "adaboost" (the AdaBoost exponential loss for 0-1
          outcomes), "poisson" (count outcomes), "coxph" (right
          censored observations), "quantile", or "pairwise" (ranking
          measure using the LambdaMart algorithm).



          

     







\section{Nouns}
cross-entropy(deviance)\\
feedforward neural network\\
back propagation algorithm\\
Markov random field\\
Restricted Boltzman Machines\\
energy function\\
contrastive divergence\\
generalized cross-validation\\
Adaboost.M1 algrithm\\
forward stagewise additive modeling\\
PRSS, Penalized sum of squares\\
backfitting\\
classification Error rate(Misclassification rate)\\
Gini Index\\
Cross-Entropy or Deviance\\
softmax function\\
Parzen estimate(Kernel)\\
varying coefficient model\\
Nadaraya-Watson kernel-weight average\\
locally weighted linear regression\\
Epanechnikov/Tri-Cube kernel\\
Gaussian kernel\\
Nearest Neighbor kernel\\
locally polynomial regression\\
stochastic search variable selection(SSVS)\\


































% You can even have references
%\rule{0.3\linewidth}{0.25pt}
%\scriptsize
%\bibliographystyle{abstract}
%\bibliography{refFile}
\end{multicols}
\end{document}

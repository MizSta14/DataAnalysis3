lines(age.grid,pfit,lwd=2, col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
table(cut(age,4))
fit=lm(wage~cut(age,4),data=Wage)
coef(summary(fit))
# Splines
library(splines)
fit=lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
pred=predict(fit,newdata=list(age=age.grid),se=T)
plot(age,wage,col="gray")
lines(age.grid,pred$fit,lwd=2)
lines(age.grid,pred$fit+2*pred$se,lty="dashed")
lines(age.grid,pred$fit-2*pred$se,lty="dashed")
dim(bs(age,knots=c(25,40,60)))
dim(bs(age,df=6))
attr(bs(age,df=6),"knots")
fit2=lm(wage~ns(age,df=4),data=Wage)
pred2=predict(fit2,newdata=list(age=age.grid),se=T)
lines(age.grid, pred2$fit,col="red",lwd=2)
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Smoothing Spline")
fit=smooth.spline(age,wage,df=16)
fit2=smooth.spline(age,wage,cv=TRUE)
fit2$df
lines(fit,col="red",lwd=2)
lines(fit2,col="blue",lwd=2)
legend("topright",legend=c("16 DF","6.8 DF"),col=c("red","blue"),lty=1,lwd=2,cex=.8)
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Local Regression")
fit=loess(wage~age,span=.2,data=Wage)
fit2=loess(wage~age,span=.5,data=Wage)
lines(age.grid,predict(fit,data.frame(age=age.grid)),col="red",lwd=2)
lines(age.grid,predict(fit2,data.frame(age=age.grid)),col="blue",lwd=2)
legend("topright",legend=c("Span=0.2","Span=0.5"),col=c("red","blue"),lty=1,lwd=2,cex=.8)
# GAMs
gam1=lm(wage~ns(year,4)+ns(age,5)+education,data=Wage)
library(gam)
gam.m3=gam(wage~s(year,4)+s(age,5)+education,data=Wage)
par(mfrow=c(1,3))
plot(gam.m3, se=TRUE,col="blue")
plot.gam(gam1, se=TRUE, col="red")
gam.m1=gam(wage~s(age,5)+education,data=Wage)
gam.m2=gam(wage~year+s(age,5)+education,data=Wage)
anova(gam.m1,gam.m2,gam.m3,test="F")
summary(gam.m3)
preds=predict(gam.m2,newdata=Wage)
gam.lo=gam(wage~s(year,df=4)+lo(age,span=0.7)+education,data=Wage)
plot.gam(gam.lo, se=TRUE, col="green")
gam.lo.i=gam(wage~lo(year,age,span=0.5)+education,data=Wage)
library(akima)
plot(gam.lo.i)
gam.lr=gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage)
par(mfrow=c(1,3))
plot(gam.lr,se=T,col="green")
table(education,I(wage>250))
gam.lr.s=gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage,subset=(education!="1. < HS Grad"))
plot(gam.lr.s,se=T,col="green")
View(se.bands)
x <- factor(c("a", "b"))
x
x[1]
x[1] == "a"
as.numeric(x)
as.numeric("3")
as.numeric("a")
?read.table
?default.stringsAsFactors
?read_csv
??read_csv
install.packages("readr")
read.csv
?read.csv
library(mgcv)
s <- mgcv:::s
install.packages("XGboost")
install.packages("xgboost")
?xgboost
?"xgboost"
library(xgboost)
?"xgboost"
setwd("~/Documents/git/DataAnalysis3")
rm(list = ls())
library(tree)
library(ISLR)
library(ggplot2)
library(MASS)
library(randomForest)
library(gbm)
library(earth)
library(mda)
library(plotmo)
par(mfrow = c(1, 1))
folds <- 10
#1
p <- seq(0, 1, 0.01)
error <- 1-apply(cbind(p, 1 - p), 1, max)
gini <- 2*p*(1 - p)
entropy <- -p * log(p) - (1 - p) * log(1 - p)
plot(c(0, 1.5), c(0, 1),
type = "n",
xlab = "p",
ylab = "index")
lines(p, entropy, lty = 1)
lines(p, gini, col = "red", lty = 2)
lines(p, error, col = "blue", lty = 3)
legend(0.8, 1,
c("Entropy", "Gini index", "Classification Error"),
lty=c(1, 2, 3),
col = c("black", "red", "blue"),
cex = .7)
#2
probs <- c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)
vote <- ifelse(sum(probs > 0.5) > length(probs)/2, TRUE, FALSE)
average <- ifelse(mean(probs) > 0.5, TRUE, FALSE)
#3
set.seed(1)
train_size <- 200
train <- sample(1:nrow(Carseats),
size = train_size)
carseats_train <- Carseats[train, ]
carseats_test <- Carseats[-train, ]
#(a)
tree_carseats <- tree(Sales ~ .,
data = carseats_train)
tree_pred=predict(tree_carseats,
carseats_test)
summary(tree_carseats)
plot(tree_carseats)
text(tree_carseats,
pretty = 0,
cex = .7)
tree_test_mse <- mean((tree_pred - carseats_test$Sales)^2)
#(b)
cv_carseats=cv.tree(tree_carseats)
par(mfrow = c(1, 2))
plot(cv_carseats$size,cv_carseats$dev,
type="b")
plot(cv_carseats$k,cv_carseats$dev,
type="b")
par(mfrow = c(1, 1))
best_tree_size <- cv_carseats$size[which.min(cv_carseats$dev)]
prune_carseats <- prune.tree(tree_carseats,
best = best_tree_size)
plot(prune_carseats)
text(prune_carseats,
pretty = 0)
prune_tree_pred <- predict(prune_carseats,
carseats_test)
prune_test_mse <- mean((prune_tree_pred - carseats_test$Sales)^2)
#(c)
bag_carseats <- randomForest(Sales ~ .,
data = carseats_train,
mtry = ncol(carseats_train) - 1,
importance = TRUE)
bag_pred <- predict(bag_carseats,
newdata = carseats_test)
bag_test_mse <- mean((bag_pred - carseats_test$Sales)^2)
important_var <- names(sort(importance(bag_carseats)[, 1],
decreasing = TRUE))[c(1, 2)]
importance(bag_carseats)
varImpPlot(bag_carseats,
main = "Importance Plot")
#(d)
boost_cv_sample <- sample(1:folds,
nrow(carseats_train),
replace = TRUE)
B <- seq(1000, 5000, by = 1000)
d <- 1:4
lambda <- c(0.001, 0.01)
boost_cv_mse <- array(dim = c(length(B), length(d), length(lambda)),
dimnames = list(B, d, lambda))
temp_mse <- numeric(10)
ptm <- proc.time()
for (i in 1:length(B)){
for (j in 1:length(d)){
for (k in 1:length(lambda)){
for (n in 1:folds){
boost_carseats <- gbm(Sales ~ .,
data = carseats_train[boost_cv_sample != n, ],
distribution = "gaussian",
n.trees = B[i],
interaction.depth = d[j],
shrinkage = lambda[k])
boost_pred <- predict(boost_carseats,
newdata = carseats_train[boost_cv_sample == n, ],
n.trees = B[i])
temp_mse[n] <- mean((boost_pred - carseats_train[boost_cv_sample == n, "Sales"])^2)
}
boost_cv_mse[i, j, k] <- mean(temp_mse)
}
}
}
working_time <- proc.time() - ptm
Best_B <- as.numeric(dimnames(boost_cv_mse)[[1]][which(boost_cv_mse == min(boost_cv_mse), arr.ind = TRUE)[1, 1]])
Best_d <- as.numeric(dimnames(boost_cv_mse)[[2]][which(boost_cv_mse == min(boost_cv_mse), arr.ind = TRUE)[1, 2]])
Best_lambda <- Best.B <- as.numeric(dimnames(boost_cv_mse)[[3]][which(boost_cv_mse == min(boost_cv_mse), arr.ind = TRUE)[1, 3]])
boost_carseats <- gbm(Sales ~ .,
data = carseats_train,
distribution = "gaussian",
n.trees = Best_B,
interaction.depth = Best_d,
shrinkage = Best_lambda)
boost_pred <- predict(boost_carseats,
newdata = carseats_test,
n.trees = Best_B)
boost_mse <- mean((boost_pred - carseats_test$Sales)^2)
#(e)
max_m <- ncol(Carseats) - 1
rf_mse <- numeric(max_m)
names(rf_mse) <- 1:max_m
for (i in 1:max_m){
rf_carseats <- randomForest(Sales ~ .,
data = carseats_train,
mtry = i,
importance = TRUE)
rf_pred = predict(rf_carseats,
newdata = carseats_test)
rf_mse[i] <- mean((rf_pred-carseats_test$Sales)^2)
}
#(f)
mars_carseats <- earth(Sales ~ .,
data = carseats_train,
pmethod = "cv",
nfold = 10)
mars_pred <- predict(mars_carseats,
newdata = carseats_test)
mars_mse <- mean((mars_pred-carseats_test$Sales)^2)
#4
set.seed(1)
#tree
tree_pima <- tree(type ~ .,
data = Pima.tr)
cv_tree_pima <- cv.tree(tree_pima,
FUN = prune.misclass)
best_tree_size <- cv_tree_pima$size[which.min(cv_tree_pima$dev)]
prune_pima <- prune.tree(tree_pima,
best = best_tree_size)
tree_pred=predict(prune_pima,
Pima.te,
type = "class")
summary(prune_pima)
plot(prune_pima)
text(prune_pima,
pretty = 0)
tree_test_mse <- (table(tree_pred, Pima.te$type)[1, 2] +
table(tree_pred, Pima.te$type)[2,1]) / nrow(Pima.te)
#bagging
bag_pima <- randomForest(type ~ .,
data = Pima.tr,
mtry = ncol(Pima.tr) - 1,
importance = TRUE)
bag_pred <- predict(bag_pima,
newdata = Pima.te)
bag_test_mse <- (table(tree_pred, Pima.te$type)[1, 2] +
table(bag_pred, Pima.te$type)[2,1]) / nrow(Pima.te)
important_var <- names(sort(importance(bag_pima)[, 2],
decreasing = TRUE))[c(1, 2)]
importance(bag_pima)
varImpPlot(bag_pima,
main = "Importance Plot")
#boosting
boost_cv_sample <- sample(1:folds,
nrow(Pima.tr),
replace = TRUE)
B <- seq(1000, 5000, by = 2000)
d <- 1:3
lambda <- c(0.001, 0.01)
boost_cv_mse <- array(dim = c(length(B), length(d), length(lambda)),
dimnames = list(B, d, lambda))
temp_mse <- numeric(10)
ptm <- proc.time()
for (i in 1:length(B)){
for (j in 1:length(d)){
for (k in 1:length(lambda)){
for (n in 1:folds){
boost_pima <- gbm(as.numeric(type) - 1 ~ .,
data = Pima.tr[boost_cv_sample != n, ],
distribution = "bernoulli",
n.trees = B[i],
interaction.depth = d[j],
shrinkage = lambda[k])
boost_pred <- predict(boost_pima,
newdata = Pima.tr[boost_cv_sample == n, ],
n.trees = B[i],
type = "response")
boost_pred <- ifelse(boost_pred > 0.5, "Yes", "No")
temp_mse[n] <- attr(confusion(boost_pred, Pima.tr[boost_cv_sample == n, "type"]),
"error")
}
boost_cv_mse[i, j, k] <- mean(temp_mse)
}
}
}
working_time_2 <- proc.time() - ptm
Best_B <- as.numeric(dimnames(boost_cv_mse)[[1]][which(boost_cv_mse == min(boost_cv_mse), arr.ind = TRUE)[1, 1]])
Best_d <- as.numeric(dimnames(boost_cv_mse)[[2]][which(boost_cv_mse == min(boost_cv_mse), arr.ind = TRUE)[1, 2]])
Best_lambda <- Best.B <- as.numeric(dimnames(boost_cv_mse)[[3]][which(boost_cv_mse == min(boost_cv_mse), arr.ind = TRUE)[1, 3]])
boost_pima <- gbm(as.numeric(type) - 1 ~ .,
data = Pima.tr,
distribution = "bernoulli",
n.trees = Best_B,
interaction.depth = Best_d,
shrinkage = Best_lambda)
boost_pred <- predict(boost_pima,
newdata = Pima.te,
n.trees = Best_B,
type = "response")
boost_pred <- ifelse(boost_pred > 0.5, "Yes", "No")
boost_test_mse <- attr(confusion(boost_pred, Pima.te$type),
"error")
#random forest
max_m <- ncol(Pima.tr) - 1
rf_mse <- numeric(max_m)
names(rf_mse) <- 1:max_m
for (i in 1:max_m){
rf_pima <- randomForest(type ~ .,
data = Pima.tr,
mtry = i,
importance = TRUE)
rf_pred = predict(rf_pima,
newdata = Pima.te)
rf_mse[i] <- attr(confusion(rf_pred, Pima.te$type),
"error")
}
rf_test_mse <- rf_mse[which.min(rf_mse)]
# mars
mars_pima <- earth(type ~ .,
data = Pima.tr,
pmethod = "cv",
nfold = 10)
mars_pred <- predict(mars_pima,
newdata = Pima.te)
mars_pred <- ifelse(mars_pred[, 1] > 0.5, "Yes", "No")
mars_test_mse <- attr(confusion(mars_pred, Pima.te$type),
"error")
all_error_rate <- list(tree_test_mse,
bag_test_mse,
boost_test_mse,
rf_test_mse,
mars_test_mse)
getwd()
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
sum(w*x)/sum(w)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)
data(mtcars)
lm(mpg~weight, data = mtcars)
View(mtcars)
lm(mpg~wt, data = mtcars)
iter <- 12
try(if(iter > 10) stop("too many iterations"))
max_m <- ncol(Carseats) - 1
rf_mse <- numeric(max_m)
names(rf_mse) <- 1:max_m
for (i in 1:max_m){
rf_carseats <- randomForest(Sales ~ .,
data = carseats_train,
mtry = i,
importance = TRUE)
rf_pred = predict(rf_carseats,
newdata = carseats_test)
rf_mse[i] <- mean((rf_pred-carseats_test$Sales)^2)
}
rf_mse
plot(rf_mse, type = "b")
importance(rf_carseats)
class(rf_carseats)
str(rf_carseats)
class(importance(rf_carseats))
dim(importance(rf_carseats))
rf_importances <- rep(matrix(, 10, 2), max_m)
rf_importances
rf_importances[1]
rf_importances <- rep(list(matrix(, 10, 2)), max_m)
rf_importances
rf_importances[1]
rf_importances[[1]]
class(rf_importances[[1]])
max_m <- ncol(Carseats) - 1
rf_mse <- numeric(max_m)
names(rf_mse) <- 1:max_m
rf_importances <- rep(list(matrix(NA, 10, 2)), max_m)
names(rf_importances) <- 1:max_m
for (i in 1:max_m){
rf_carseats <- randomForest(Sales ~ .,
data = carseats_train,
mtry = i,
importance = TRUE)
rf_importances[[i]] <- importance(rf_carseats)
rf_pred = predict(rf_carseats,
newdata = carseats_test)
rf_mse[i] <- mean((rf_pred-carseats_test$Sales)^2)
}
rf_importances
names(rf_importances[[1]][, 1])
names(rf_importances[[1:10]][, 1])
sum(rf_importances[[1]][, 2])
sum(abs(rf_importances[[1]][, 1]))
names(rf_importances[[1]][, 1])[which.max(rf_importances[[1]][, 1])]
for (i in 1:10){}
for (i in 1:10){
names(rf_importances[[i]][, 1])[which.max(rf_importances[[i]][, 1])]}
for (i in 1:10){
most[i] <- names(rf_importances[[i]][, 1])[which.max(rf_importances[[i]][, 1])]}
most <- NA
most
for (i in 1:10){
most[i] <- names(rf_importances[[i]][, 1])[which.max(rf_importances[[i]][, 1])]}
most
max_m
for (i in 1:10){
most[i] <- names(sort(rf_importances[[i]][, 1],
decreasing = TRUE))[c(1, 2)]}
most
most <- NA
for (i in 1:10){
most[i] <- names(sort(rf_importances[[i]][, 1],
decreasing = TRUE))[c(1, 2)]}
most
most <- rep(list(c(NA, NA)), 10)
for (i in 1:10){
most[[i]] <- names(sort(rf_importances[[i]][, 1],
decreasing = TRUE))[c(1, 2)]}
most
plot(rf_mse,
main = "The relation between MSE and then number of varibles in each split",
type = "b",
xlab = "m",
ylab = "MSE")
plot(rf_mse,
main = "The relation between MSE and m",
type = "b",
xlab = "m",
ylab = "MSE")
mars_carseats
summary(mars_carseats)
mars_carseats$gcv
warnings()
warning()
mars_carseats$gcv
mars_carseats$rss
mars_mse
nrow(Carseats)
mars_carseats$rss/200
mars_carseats$rss/199
mars_carseats$cuts
mars_carseats$selected.terms
str(mars_carseats$cuts)
dimnames(mars_carseats$cuts)
dimnames(mars_carseats$cuts)[[1]][mars_carseats$selected.terms
]
str(summary(mars_carseats))
?summary.earth
summary(mars_carseats)$strings
summary(mars_carseats)$fixed.point
summary(mars_carseats)$details
?earth
mars_carseats <- earth(Sales ~ .,
data = carseats_train,
pmethod = "cv",
nfold = 10,
degree = 2)
mars_pred <- predict(mars_carseats,
newdata = carseats_test)
mars_mse <- mean((mars_pred-carseats_test$Sales)^2)
summary()
summary(mars_carseats)
mars_mse
mars_carseats <- earth(Sales ~ .,
data = carseats_train,
pmethod = "cv",
nfold = 10)
mars_pred <- predict(mars_carseats,
newdata = carseats_test)
mars_mse <- mean((mars_pred-carseats_test$Sales)^2)
mars_mse
mars_carseats <- earth(Sales ~ .,
data = carseats_train,
pmethod = "cv",
nfold = 10,
degree = 3)
mars_pred <- predict(mars_carseats,
newdata = carseats_test)
mars_mse <- mean((mars_pred-carseats_test$Sales)^2)
mars_mse
?randomForest
all_error_rate <- c(tree_test_mse,
bag_test_mse,
boost_test_mse,
rf_test_mse,
mars_test_mse)
all_error_rate
all_error_rate <- c(tree_test_mse,
bag_test_mse,
boost_test_mse,
rf_test_mse,
mars_test_mse)
names(all_error_rate) <- c("Simple Tree",
"Tree with bagging",
"Tree with boostin",
"Tree with random forest",
"MARS")
all_error_rate
names(all_error_rate)[which.min(all_error_rate)]
rf_pima <- randomForest(type ~ .,
data = Pima.tr,
mtry = 1,
importance = TRUE)
summary(rf_pima)
rf_pima
